{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajits-github/Exploring_NN_with_Numpy/blob/main/Neural_network_with_numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fnmj6ihdtcXW"
      },
      "source": [
        "# Let's code a Neural Network in plain NumPy\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU6y2UqEtcXY"
      },
      "source": [
        "Utilizing powerful frameworks such as Keras, TensorFlow, or PyTorch enables us to rapidly construct intricate models. However, it is essential to delve deeper and grasp the underlying principles. For this purpose, we will now endeavor to leverage our knowledge to construct a fully functional neural network solely with NumPy. We will then apply it to solve straightforward classification tasks and compare its performance against a model created with Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FEX5xvTtcXZ"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MaPBWGstcXZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from IPython.display import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pp_mnl0ntcXZ"
      },
      "source": [
        "## Network architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3JtOaPotcXa"
      },
      "source": [
        "![Network architecture](./supporting_visualizations/nn_architecture.png)\n",
        "\n",
        "<b>Figure 1.</b> Example of dense neural network architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf_hOlfltcXa"
      },
      "source": [
        "## First things first"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBsiquO5tcXa"
      },
      "source": [
        "Before we start programming, let's stop for a moment and prepare a basic roadmap. Our goal is to create a program capable of creating a densely connected neural network with the specified architecture (number and size of layers and appropriate activation function). An example of such a network is presented in Figure 1. Above all, we must be able to our network and make predictions using it.\n",
        "\n",
        "![Roadmap](./supporting_visualizations/blueprint.gif)\n",
        "\n",
        "<b>Figure 2.</b> Neural network blueprint\n",
        "\n",
        "Diagram above shows what operations will have to be performed during the training of our neural network. It also shows how many parameters we will have to update and read at different stages of a single iteration. Building the right data structure and skillfully managing its state is the most difficult part of our task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwBwZHi3tcXa"
      },
      "source": [
        "## Initiation of neural network layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "br0uES0OtcXa"
      },
      "source": [
        "![Parameters sizes](./supporting_visualizations/params_sizes.png)\n",
        "\n",
        "<b>Figure 3.</b> Dimensions of weight matrix W and bias vector b for layer l.\n",
        "\n",
        "Let's start with by initiating weight matrix W and bias vector b for each layer. In Figure 3 I have prepared a small cheatsheet, which will help us to asign the appropriate dimensions for these coefficients. Superscript [l] denotes the index of the current layer (counted from 1). I assumed that the information describing the NN architecture will be delivered to our program in the form of list. Each item in the list is a dictionary describing the basic parameters of a single network layer: input_dim - the size of the signal vector supplied as an input for the layer, output_dim - the size of the activation vector obtained at the output of the layer and activation - the activation function to be used inside the layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyzOFeiytcXb"
      },
      "outputs": [],
      "source": [
        "NN_ARCHITECTURE = [\n",
        "    {\"input_dim\": 2, \"output_dim\": 25, \"activation\": \"relu\"},\n",
        "    {\"input_dim\": 25, \"output_dim\": 50, \"activation\": \"relu\"},\n",
        "    {\"input_dim\": 50, \"output_dim\": 50, \"activation\": \"relu\"},\n",
        "    {\"input_dim\": 50, \"output_dim\": 25, \"activation\": \"relu\"},\n",
        "    {\"input_dim\": 25, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3o9MKittcXb"
      },
      "source": [
        "## Initiation of parameter values for each layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apj7RmqVtcXb"
      },
      "source": [
        "Let's finally focus on the main task that we have to accomplish in this part - the initiation of layers parameters. Initial weights values cannot be equal because it leads to breaking symmetry problem. Basically, if all weights are the same, no matter what was the input X, all units in hidden layer will be the same too. In a way, we got stuck in the initial state without any hope for escape, no matter how long will we train our model and how deep our network is. The use of small values increases the efficiency of our algorithm during first iterations. Looking at the graph of the sigmoid function, shown in Figure 4, we can see that it reaches the highest derivative value for numbers close to zero, which has significant effect on the speed of learning of our NN. All in all parameter initiation using small random numbers is simple approach, but it guarantees good enough starting point for out algorithm. Prepared parameters values are stored in a python dictionary with a key that uniquely identifies to which layer they belong. The dictionary is returned at the end of the function, so we can use it in the next stages of our algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwST7contcXb"
      },
      "outputs": [],
      "source": [
        "def init_layers(nn_architecture, seed = 99):\n",
        "    # random seed initiation\n",
        "    np.random.seed(seed)\n",
        "    # number of layers in our neural network\n",
        "    number_of_layers = len(nn_architecture)\n",
        "    # parameters storage initiation\n",
        "    params_values = {}\n",
        "\n",
        "    # iteration over network layers\n",
        "    for idx, layer in enumerate(nn_architecture):\n",
        "        # we number network layers from 1\n",
        "        layer_idx = idx + 1\n",
        "\n",
        "        # extracting the number of units in layers\n",
        "        layer_input_size = layer[\"input_dim\"]\n",
        "        layer_output_size = layer[\"output_dim\"]\n",
        "\n",
        "        # initiating the values of the W matrix\n",
        "        # and vector b for subsequent layers\n",
        "        params_values['W' + str(layer_idx)] = np.random.randn(\n",
        "            layer_output_size, layer_input_size) * 0.1\n",
        "        params_values['b' + str(layer_idx)] = np.random.randn(\n",
        "            layer_output_size, 1) * 0.1\n",
        "\n",
        "    return params_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WzknetqtcXb"
      },
      "source": [
        "## Activation functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7W3Z6UWtcXc"
      },
      "source": [
        "Amongst all the functions that we will use today, there are a few very simple but powerful ones. Activation functions can be written in a single line of code, but they give the neural nets non-linearity and therefore the expressiveness that they need. \"Without them, our neural network would become a combination of linear functions, so it would be just a linear function itself.\" There are many activation functions, but in this project I decided to provide the possibility of using two of them - sigmoid and ReLU. We also have to prepare their derivatives in order to be able to go full circle and pass both forward and backward propagation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTKE6DoGtcXc"
      },
      "source": [
        "![Activations](./supporting_visualizations/activations.gif)\n",
        "\n",
        "<b>Figure 4.</b> Activation functions used in the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2b7eS9VtcXc"
      },
      "outputs": [],
      "source": [
        "def sigmoid(Z):\n",
        "    return 1/(1+np.exp(-Z))\n",
        "\n",
        "def relu(Z):\n",
        "    return np.maximum(0,Z)\n",
        "\n",
        "def sigmoid_backward(dA, Z):\n",
        "    sig = sigmoid(Z)\n",
        "    return dA * sig * (1 - sig)\n",
        "\n",
        "def relu_backward(dA, Z):\n",
        "    dZ = np.array(dA, copy = True)\n",
        "    dZ[Z <= 0] = 0;\n",
        "    return dZ;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NB27pbHtcXc"
      },
      "source": [
        "## Single layer forward propagation step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-SDmI0FtcXc"
      },
      "source": [
        "This part of the code is probably the most straightforward and easy to understand . Given input signal from previous layer, we compute affine transformation Z and then apply selected activation function. By using NumPy, we can leverage vectorization - performing matrix operations, for whole layer and whole batch of examples at once. This eliminates iteration and significantly speeds up our calculations. In addition to the calculated matrix A, our function also returns an intermediate value of Z. What for? The answer is shown in Figure 2. We will need Z during the backward step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqNJ5SnitcXc"
      },
      "source": [
        "$$\\boldsymbol{Z}^{[l]} = \\boldsymbol{W}^{[l]} \\cdot \\boldsymbol{A}^{[l-1]} + \\boldsymbol{b}^{[l]}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvWSlo3RtcXc"
      },
      "source": [
        "$$\\boldsymbol{A}^{[l]} = g^{[l]}(\\boldsymbol{Z}^{[l]})$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sv56_0LKtcXc"
      },
      "outputs": [],
      "source": [
        "def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n",
        "    # calculation of the input value for the activation function\n",
        "    Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
        "\n",
        "    # selection of activation function\n",
        "    if activation is \"relu\":\n",
        "        activation_func = relu\n",
        "    elif activation is \"sigmoid\":\n",
        "        activation_func = sigmoid\n",
        "    else:\n",
        "        raise Exception('Non-supported activation function')\n",
        "\n",
        "    # return of calculated activation A and the intermediate Z matrix\n",
        "    return activation_func(Z_curr), Z_curr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXUA30HktcXc"
      },
      "source": [
        "![Matrix sizes 2](./supporting_visualizations/matrix_sizes_2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr6z93mltcXc"
      },
      "source": [
        "## Full forward propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aWN6Ff9tcXd"
      },
      "source": [
        "With the single_layer_forward_propagation function completed, we can easily build a whole step forward. This is a slightly more complex function, whose role is not only to perform predictions but also to organize the collection of intermediate values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNOgR0TjtcXd"
      },
      "outputs": [],
      "source": [
        "def full_forward_propagation(X, params_values, nn_architecture):\n",
        "    # creating a temporary memory to store the information needed for a backward step\n",
        "    memory = {}\n",
        "    # X vector is the activation for layer 0\n",
        "    A_curr = X\n",
        "\n",
        "    # iteration over network layers\n",
        "    for idx, layer in enumerate(nn_architecture):\n",
        "        # we number network layers from 1\n",
        "        layer_idx = idx + 1\n",
        "        # transfer the activation from the previous iteration\n",
        "        A_prev = A_curr\n",
        "\n",
        "        # extraction of the activation function for the current layer\n",
        "        activ_function_curr = layer[\"activation\"]\n",
        "        # extraction of W for the current layer\n",
        "        W_curr = params_values[\"W\" + str(layer_idx)]\n",
        "        # extraction of b for the current layer\n",
        "        b_curr = params_values[\"b\" + str(layer_idx)]\n",
        "        # calculation of activation for the current layer\n",
        "        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n",
        "\n",
        "        # saving calculated values in the memory\n",
        "        memory[\"A\" + str(idx)] = A_prev\n",
        "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
        "\n",
        "    # return of prediction vector and a dictionary containing intermediate values\n",
        "    return A_curr, memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLtKWGRjtcXd"
      },
      "source": [
        "## Calculating cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNZIRdkotcXd"
      },
      "source": [
        "In order to monitor our progress and make sure that we are moving in desired direction, we should routinely calculate the value of the loss function. \"Generally speaking, the loss function is designed to show how far we are from the 'ideal' solution.\" It is selected according to the problem we plan to solve, and frameworks such as Keras have many options to choose from. Because I am planning to test our NN for the classification of points between two classes, I decided to use binary crossentropy, which is defined by the following formulas. In order to give us more information on how our neural network is coping with the task, I have also decided to implement a function that will calculate accuracy for us.\n",
        "\n",
        "![Cost Function](./supporting_visualizations/cost_function.gif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-cllEY_tcXd"
      },
      "outputs": [],
      "source": [
        "def get_cost_value(Y_hat, Y):\n",
        "    # number of examples\n",
        "    m = Y_hat.shape[1]\n",
        "    # calculation of the cost according to the formula\n",
        "    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
        "    return np.squeeze(cost)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pL7Es8WtcXd"
      },
      "source": [
        "## Calculating accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSsBFcT0tcXd"
      },
      "outputs": [],
      "source": [
        "# an auxiliary function that converts probability into class\n",
        "def convert_prob_into_class(probs):\n",
        "    probs_ = np.copy(probs)\n",
        "    probs_[probs_ > 0.5] = 1\n",
        "    probs_[probs_ <= 0.5] = 0\n",
        "    return probs_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQEofKD9tcXd"
      },
      "outputs": [],
      "source": [
        "def get_accuracy_value(Y_hat, Y):\n",
        "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
        "    return (Y_hat_ == Y).all(axis=0).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmyK3Op7tcXd"
      },
      "source": [
        "## Single layer backward propagation step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKxmWRRntcXd"
      },
      "source": [
        "Sadly, backward propagation is regarded by many inexperienced deep learning enthusiasts as algorithm that is intimidating and difficult to understand. The combination of differential calculus and linear algebra very often deters people who do not have a solid mathematical training.\n",
        "\n",
        "Often people confuse backward propaganda with gradient descent, but in fact these are two separate matters. The purpose of the first one is to calculate the gradient effectively, whereas the second one is to use the calculated gradient to optimize. In NN, we calculate the gradient of the cost function (discussed earlier) in respect to parameters, but backpropagation can be used to calculate derivatives of any function. The essence of this algorithm is the recursive use of a chain rule known from differential calculus - calculate a derivative of functions created by assembling other functions, whose derivatives we already know. This process - for one network layer - is described by the following formulas. Unfortunately, due to the fact that this article focuses mainly on practical implementation, I'll omit the derivation. Looking at the formulas, it becomes obvious why we decided to remember the values of the A and Z matrices for intermediate layers in a forward step.\n",
        "\n",
        "$$\\boldsymbol{dW}^{[l]} = \\frac{\\partial L }{\\partial \\boldsymbol{W}^{[l]}} = \\frac{1}{m} \\boldsymbol{dZ}^{[l]} \\boldsymbol{A}^{[l-1] T}$$\n",
        "\n",
        "$$\\boldsymbol{db}^{[l]} = \\frac{\\partial L }{\\partial \\boldsymbol{b}^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} \\boldsymbol{dZ}^{[l](i)}$$\n",
        "\n",
        "$$\\boldsymbol{dA}^{[l-1]} = \\frac{\\partial L }{\\partial \\boldsymbol{A}^{[l-1]}} = \\boldsymbol{W}^{[l] T} \\boldsymbol{dZ}^{[l]}$$\n",
        "\n",
        "$$\\boldsymbol{dZ}^{[l]} = \\boldsymbol{dA}^{[l]} * g'(\\boldsymbol{Z}^{[l]})$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "la9kKBIytcXe"
      },
      "outputs": [],
      "source": [
        "def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
        "    # number of examples\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    # selection of activation function\n",
        "    if activation is \"relu\":\n",
        "        backward_activation_func = relu_backward\n",
        "    elif activation is \"sigmoid\":\n",
        "        backward_activation_func = sigmoid_backward\n",
        "    else:\n",
        "        raise Exception('Non-supported activation function')\n",
        "\n",
        "    # calculation of the activation function derivative\n",
        "    dZ_curr = backward_activation_func(dA_curr, Z_curr)\n",
        "\n",
        "    # derivative of the matrix W\n",
        "    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n",
        "    # derivative of the vector b\n",
        "    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
        "    # derivative of the matrix A_prev\n",
        "    dA_prev = np.dot(W_curr.T, dZ_curr)\n",
        "\n",
        "    return dA_prev, dW_curr, db_curr"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}